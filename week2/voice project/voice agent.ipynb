{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "# portaudio is required by the sounddevice python package\n",
    "!apt-get install -y portaudio19-dev python3-pyaudio\n",
    "# usual python dependency installs\n",
    "!pip install -qU \\\n",
    "    \"matplotlib==3.10.1\" \\\n",
    "    \"openai-agents[voice]==0.1.0\" \\\n",
    "    \"sounddevice==0.5.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'name': 'default',\n",
       "  'index': 6,\n",
       "  'hostapi': 0,\n",
       "  'max_input_channels': 32,\n",
       "  'max_output_channels': 32,\n",
       "  'default_low_input_latency': 0.008684807256235827,\n",
       "  'default_low_output_latency': 0.008684807256235827,\n",
       "  'default_high_input_latency': 0.034807256235827665,\n",
       "  'default_high_output_latency': 0.034807256235827665,\n",
       "  'default_samplerate': 44100.0},\n",
       " {'name': 'default',\n",
       "  'index': 6,\n",
       "  'hostapi': 0,\n",
       "  'max_input_channels': 32,\n",
       "  'max_output_channels': 32,\n",
       "  'default_low_input_latency': 0.008684807256235827,\n",
       "  'default_low_output_latency': 0.008684807256235827,\n",
       "  'default_high_input_latency': 0.034807256235827665,\n",
       "  'default_high_output_latency': 0.034807256235827665,\n",
       "  'default_samplerate': 44100.0})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "\n",
    "input_device = sd.query_devices(kind='input')\n",
    "output_device = sd.query_devices(kind='output')\n",
    "\n",
    "input_device, output_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the sample rate for these devices via the `default_samplerate` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44100.0, 44100.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_samplerate = sd.query_devices(kind='input')['default_samplerate']\n",
    "out_samplerate = sd.query_devices(kind='output')['default_samplerate']\n",
    "\n",
    "in_samplerate, out_samplerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorded_chunks = []\n",
    "\n",
    "# start streaming from microphone until Enter is pressed\n",
    "with sd.InputStream(\n",
    "    samplerate=in_samplerate,\n",
    "    channels=1,\n",
    "    dtype='int16',\n",
    "    callback=lambda indata, frames, time, status: recorded_chunks.append(indata.copy())\n",
    "):\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(recorded_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84445, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "audio_buffer = np.concatenate(recorded_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.play(audio_buffer, samplerate=out_samplerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioInput(buffer=array([[-26663],\n",
       "       [-26740],\n",
       "       [-26851],\n",
       "       ...,\n",
       "       [    -3],\n",
       "       [    26],\n",
       "       [    16]], shape=(84445, 1), dtype=int16), frame_rate=44100.0, sample_width=2, channels=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from agents.voice import AudioInput\n",
    "\n",
    "audio_input = AudioInput(\n",
    "    buffer=audio_buffer,\n",
    "    frame_rate=in_samplerate,\n",
    "    channels=audio_buffer.shape[1], \n",
    ")\n",
    "audio_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=(\n",
    "        \"Repeat the user's question back to them, and then answer it. Note that the user is \"\n",
    "        \"speaking to you via a voice interface, although you are reading and writing text to \"\n",
    "        \"respond. Nonetheless, ensure that your written response is easily translatable to voice.\"\n",
    "        \"Also Make Your Voice Human Like and Stable\"\n",
    "        \"Speak in a friendly manner and hindi language\"\n",
    "    ),\n",
    "    model=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.voice import SingleAgentVoiceWorkflow\n",
    "\n",
    "workflow = SingleAgentVoiceWorkflow(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.voice import TTSModelSettings, VoicePipelineConfig\n",
    "\n",
    "custom_tts_settings = TTSModelSettings(\n",
    "    instructions=(\n",
    "        \"Personality: upbeat, friendly, persuasive guide.\\n\"\n",
    "        \"Tone: Friendly, clear, and reassuring, creating a calm atmosphere and making \"\n",
    "        \"the listener feel confident and comfortable.\\n\"\n",
    "        \"Pronunciation: Clear, articulate, and steady, ensuring each instruction is \"\n",
    "        \"easily understood while maintaining a natural, conversational flow.\\n\"\n",
    "        \"Tempo: Speak relatively fast, include brief pauses and after before questions.\\n\"\n",
    "        \"Emotion: Warm and supportive, conveying empathy and care, ensuring the listener \"\n",
    "        \"feels guided and safe throughout the journey.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "voice_pipeline_config = VoicePipelineConfig(tts_settings=custom_tts_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.voice import VoicePipeline\n",
    "\n",
    "pipeline = VoicePipeline(workflow=workflow, config=voice_pipeline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await pipeline.run(audio_input=audio_input)\n",
    "\n",
    "response_chunks = []\n",
    "\n",
    "async for event in result.stream():\n",
    "    if event.type == \"voice_stream_event_audio\":\n",
    "        response_chunks.append(event.data)\n",
    "\n",
    "response_audio_buffer = np.concatenate(response_chunks, axis=0)\n",
    "\n",
    "openai_sample_rate = 24_000\n",
    "\n",
    "sd.play(response_audio_buffer, samplerate=openai_sample_rate)\n",
    "sd.wait() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "Assistant is responding...\n",
      "---\n",
      "Listening...\n",
      "Assistant is responding...\n",
      "---\n",
      "Listening...\n",
      "Assistant is responding...\n",
      "---\n",
      "Listening...\n",
      "Assistant is responding...\n",
      "---\n",
      "Listening...\n",
      "Assistant is responding...\n",
      "---\n",
      "Listening...\n",
      "Assistant is responding...\n",
      "---\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "async def voice_assistant_optimized():\n",
    "    while True:\n",
    "\n",
    "        cmd = input(\"Press Enter to speak (or type 'q' to exit): \")\n",
    "        if cmd.lower() == \"q\" :\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        print(\"Listening...\")\n",
    "        recorded_chunks = []\n",
    "\n",
    "        with sd.InputStream(\n",
    "            samplerate=in_samplerate,\n",
    "            channels=1,\n",
    "            dtype='int16',\n",
    "            callback=lambda indata, frames, time, status: recorded_chunks.append(indata.copy())\n",
    "        ):\n",
    "            input()\n",
    "\n",
    "        recording = np.concatenate(recorded_chunks, axis=0)\n",
    "\n",
    "        audio_input = AudioInput(buffer=recording)\n",
    "\n",
    "        result = await pipeline.run(audio_input)\n",
    "\n",
    "        response_chunks = []\n",
    "        async for event in result.stream():\n",
    "            if event.type == \"voice_stream_event_audio\":\n",
    "                response_chunks.append(event.data)\n",
    "\n",
    "        response_audio_buffer = np.concatenate(response_chunks, axis=0)\n",
    "\n",
    "        print(\"Assistant is responding...\")\n",
    "        sd.play(response_audio_buffer, samplerate=openai_sample_rate)\n",
    "        sd.wait()\n",
    "        print(\"---\")\n",
    "\n",
    "await voice_assistant_optimized()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
